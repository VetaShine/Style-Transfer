{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Neural Style Transfer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запуск обученной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Общий вид запуска программы тестирования:\n",
    "# python3 test_on_image.py --image_path <path-to-image> --checkpoint_model <path-to-checkpoint> \n",
    "\n",
    "# Конкретный пример запуска тестирования:\n",
    "# python3 test_on_image.py --image_path /Users/macbookpro/bird.jpg --checkpoint_model /Users/macbookpro/checkpoints/результаты/kalzado_10000.pth "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Программа тестирования обученной модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем необходимые для работы программы библиотеки. При запуске программы из терминала мы можем также просто импортировать из ранее созданной программы models.py класс TransformerNet для загрузки весов обученной модели. Также мы импортируем все функции из ранее созданной программы utils.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from this import s\n",
    "from models import TransformerNet\n",
    "from utils import *\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import argparse\n",
    "import os\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализация класса TransformerNet, для которого требуются дополнительные классы ResidualBlock и ConvBlock:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer network\n",
    "class TransformerNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            ConvBlock(3, 32, kernel_size = 9, stride = 1),\n",
    "            ConvBlock(32, 64, kernel_size = 3, stride = 2),\n",
    "            ConvBlock(64, 128, kernel_size = 3, stride = 2),\n",
    "            ResidualBlock(128),\n",
    "            ResidualBlock(128),\n",
    "            ResidualBlock(128),\n",
    "            ResidualBlock(128),\n",
    "            ResidualBlock(128),\n",
    "            ConvBlock(128, 64, kernel_size = 3, upsample = True),\n",
    "            ConvBlock(64, 32, kernel_size = 3, upsample = True),\n",
    "            ConvBlock(32, 3, kernel_size = 9, stride = 1, normalize = False, relu = False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Residual block\n",
    "class ResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            ConvBlock(channels, channels, kernel_size = 3, stride = 1, normalize = True, relu = True),\n",
    "            ConvBlock(channels, channels, kernel_size = 3, stride = 1, normalize = True, relu = False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x) + x\n",
    "\n",
    "\n",
    "# Convolutional block\n",
    "class ConvBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride = 1, upsample = False, normalize = True, relu = True):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.upsample = upsample\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(kernel_size // 2), nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "        )\n",
    "        self.norm = nn.InstanceNorm2d(out_channels, affine=True) if normalize else None\n",
    "        self.relu = relu\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.upsample:\n",
    "            x = F.interpolate(x, scale_factor = 2)\n",
    "        x = self.block(x)\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        if self.relu:\n",
    "            x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Требуемые функции из программы utils.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Среднее значение и стандартное отклонение, используемые для предварительно обученных моделей PyTorch\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "def style_transform(image_size = None):\n",
    "    \"\"\" Преобразование стилевых изображений \"\"\"\n",
    "    resize = [transforms.Resize(image_size)] if image_size else []\n",
    "    transform = transforms.Compose(resize + [transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
    "    return transform\n",
    "\n",
    "def denormalize(tensors):\n",
    "    \"\"\" Денормализация тензоров изображений с использованием среднего и стандартного отклонения \"\"\"\n",
    "    for c in range(3):\n",
    "        tensors[:, c].mul_(std[c]).add_(mean[c])\n",
    "    return tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В начале мы проверяем верность введённых ключей, при их неправильном вводе выводим пользователю подсказку, создаём нужные директории при их изначальном отсутствии. После определяем модель и загружаем обученные веса по пути, заданному ключом \"--checkpoint_model\". Далее загружаем контентное изображение по пути, указанному ключом \"--image_path\", проводим стилизацию этого изображения, сохраняя сгенерированное изображение на локальный компьютер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--image_path\", type = str, required = True, help = \"Path to image\")\n",
    "    parser.add_argument(\"--checkpoint_model\", type = str, required = True, help = \"Path to checkpoint model\")\n",
    "    args = parser.parse_args()\n",
    "    print(args)\n",
    "\n",
    "    os.makedirs(\"images/outputs\", exist_ok = True)\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    transform = style_transform()\n",
    "\n",
    "    # Определение модели, загрузка весов модели \n",
    "    transformer = TransformerNet().to(device)\n",
    "    transformer.load_state_dict(torch.load(args.checkpoint_model, map_location = 'cpu'))\n",
    "    transformer.eval()\n",
    "\n",
    "    # Загрузка контентного изображения\n",
    "    image_tensor = Variable(transform(Image.open(args.image_path))).to(device)\n",
    "    image_tensor = image_tensor.unsqueeze(0)\n",
    "\n",
    "    # Стилизация изображения\n",
    "    with torch.no_grad():\n",
    "        stylized_image = denormalize(transformer(image_tensor)).cpu()\n",
    "\n",
    "    # Сохранение сгенерированного изображения\n",
    "    fn = args.image_path.split(\"/\")[-1]\n",
    "    save_image(stylized_image, f\"stylized-{fn}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
